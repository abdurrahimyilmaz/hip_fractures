{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76bbae9-028a-4a0f-8e01-91f29ba6fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "# %matplotlib inline\n",
    "\n",
    "# system libraries\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# numpy and pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "\n",
    "# matplotlib and visualization libs\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from PIL import Image\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import config\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eb13df",
   "metadata": {},
   "source": [
    "## AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac839ce-e440-4ff5-96e4-dbf39c1f7adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "print(\"Num GPUs Available: \", len( config.experimental.list_physical_devices('GPU')))\n",
    "##physical_devices = config.list_physical_devices(\"GPU\")\n",
    "\n",
    "#config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "physical_devices = config.experimental.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "  try:\n",
    "    for physical_device in physical_devices:\n",
    "      config.experimental.set_memory_growth(physical_device, True)\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "    \n",
    "tf.config.set_visible_devices(physical_devices[2], 'GPU')\n",
    "\n",
    "#%%  \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          model_no=1,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(f\"../models/plots/kfold_{model_no}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0f4c4-e767-4c1a-a7e5-7f30ac0c3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# %% variables\n",
    "    \n",
    "classes = {\"normal\": 0,\n",
    "           \"implant\": 1,\n",
    "           \"fracture\": 2,\n",
    "           }\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "loss_function = \"categorical_crossentropy\"\n",
    "input_shape = (299, 299, 3)\n",
    "num_classes = 3\n",
    "verbosity = 1\n",
    "num_folds = 5\n",
    "# %%\n",
    "#model = load_model('/content/drive/MyDrive/Skin Cancer/Data_models/model.h5')\n",
    "\n",
    "x_data = load(\"DATA_PATH\")\n",
    "y_data = load(\"DATA_PATH\")\n",
    "\n",
    "print(\"0:\" + str(np.count_nonzero(y_data==0)))\n",
    "print(\"1:\" + str(np.count_nonzero(y_data==1)))\n",
    "print(\"2:\" + str(np.count_nonzero(y_data==2)))\n",
    "\n",
    "# %%\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.15, random_state = 28,stratify = y_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.1, random_state = 28, stratify = y_data)\n",
    "\n",
    "root_dir = \"ROOT_DIR\"  # Replace with your root directory\n",
    "img_size = (299, 299)\n",
    "\n",
    "X_data = []\n",
    "y_data = []\n",
    "class_names = sorted(os.listdir(root_dir))  # Get class names in sorted order\n",
    "\n",
    "for label, class_name in enumerate(class_names):\n",
    "    class_dir = os.path.join(root_dir, class_name)\n",
    "    \n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue  # Skip files, only process directories\n",
    "    \n",
    "    print(f\"Processing class: {class_name}\")\n",
    "\n",
    "    for file_name in tqdm(os.listdir(class_dir)):\n",
    "        file_path = os.path.join(class_dir, file_name)\n",
    "        try:\n",
    "            img = cv2.imread(file_path)\n",
    "            if img is None:\n",
    "                continue  # Skip unreadable images\n",
    "            \n",
    "            img = cv2.resize(img, img_size)  # Resize image\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "            X_data.append(img)\n",
    "            y_data.append(class_name)  # Store class name instead of label index\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "X_data = np.array(X_data, dtype=np.uint8)  # Convert list to numpy array\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "# Encode labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_data = label_encoder.fit_transform(y_data)\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Total images: {len(X_data)}\")\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b508625-03f6-4a5b-92e7-cfe7d8892dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], *(299, 299, 3))\n",
    "X_val = X_val.reshape(X_val.shape[0], *(299, 299, 3))\n",
    "X_test = X_test.reshape(X_test.shape[0], *(299, 299, 3))\n",
    "\n",
    "# Parse numbers as floats\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val = to_categorical(y_val, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "\n",
    "# %%\n",
    "train_datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    rotation_range=45,\n",
    "    zoom_range=0.2,  # Randomly zoom image\n",
    "    # randomly shift images horizontally (fraction of total width)\n",
    "    width_shift_range=0.2,\n",
    "    # randomly shift images vertically (fraction of total height)\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    ")\n",
    "\n",
    "train_datagen.fit(X_train)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, decay=0.0, amsgrad=False)\n",
    "\n",
    "#%%\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits = num_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24018dce-81a0-4c45-9697-70218b66c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Enable mixed precision\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3accd342-be9d-49ac-b442-d149da3619ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no = 1\n",
    "\n",
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet',\n",
    "    input_shape=input_shape,\n",
    "    include_top=False\n",
    ")\n",
    "\n",
    "## callbacks part\n",
    "mc_path = f\"../models/best_models/{base_model.name}_kfold-model{model_no}.weights.h5\"\n",
    "\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath = mc_path,\n",
    "    monitor = 'val_accuracy',\n",
    "    mode = 'max',\n",
    "    verbose = 1,\n",
    "    save_best_only = True,\n",
    "    save_weights_only = True\n",
    ")\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    factor=0.1,\n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "log_csv = CSVLogger(f'../models/model_logs/{base_model.name}_model{model_no}_logs.csv', separator = ',', append = False)\n",
    "early_stopping = EarlyStopping(monitor='loss', patience = 10)\n",
    "\n",
    "callbacks_list = [learning_rate_reduction, model_checkpoint, log_csv]\n",
    "\n",
    "\n",
    "base_modelQ4 = base_model\n",
    "\n",
    "base_modelQ4.trainable = False\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "\n",
    "q4 = base_modelQ4(inputs, training=False)\n",
    "q4 = keras.layers.Conv2D(64, (1,1), activation = 'relu', padding = 'same', name = \"pikachu\")(q4)\n",
    "q4 = keras.layers.GlobalAveragePooling2D()(q4)\n",
    "q4 = keras.layers.Dropout(0.2)(q4)\n",
    "q4 = keras.layers.Flatten()(q4)\n",
    "q4 = keras.layers.Dense(256, activation='relu')(q4)\n",
    "q4 = keras.layers.Dropout(0.2)(q4)\n",
    "q4 = keras.layers.Dense(128, activation='relu')(q4)\n",
    "q4 = keras.layers.Dropout(0.3)(q4)\n",
    "q4 = keras.layers.Dense(64, activation='relu')(q4)\n",
    "q4 = keras.layers.Dropout(0.3)(q4)\n",
    "\n",
    "\n",
    "outputs = keras.layers.Dense(num_classes, activation='softmax')(q4)\n",
    "modelQ4 = keras.Model(inputs, outputs)\n",
    "\n",
    "modelQ4.compile(\n",
    "    loss= loss_function,\n",
    "    metrics=[\"accuracy\"],\n",
    "                optimizer=optimizer)\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "epochs = 50\n",
    "modelQ4.fit(\n",
    "    train_datagen.flow(\n",
    "        X_train[train], \n",
    "        y_train[train], \n",
    "        batch_size = batch_size\n",
    "    ),\n",
    "    epochs=epochs,\n",
    "    validation_data = (X_train[val], y_train[val]),\n",
    "    verbose=1,\n",
    "    steps_per_epoch = X_train[train].shape[0] // batch_size,\n",
    "    callbacks = [learning_rate_reduction]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03b383-7f60-4f84-8461-deaa4e4c2c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Fine Tuning\n",
    "base_modelQ4.trainable = True\n",
    "new_optimizer = Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, decay=0.0, amsgrad=False)\n",
    "modelQ4.compile(\n",
    "    loss = loss_function,\n",
    "    metrics = [\"accuracy\"],\n",
    "    optimizer = new_optimizer\n",
    ")\n",
    "\n",
    "epochs = 100\n",
    "callbacks_list = [learning_rate_reduction, model_checkpoint, log_csv, early_stopping]\n",
    "modelQ4.fit(\n",
    "    train_datagen.flow(\n",
    "        X_train[train],\n",
    "        y_train[train],\n",
    "        batch_size = batch_size\n",
    "    ),\n",
    "    epochs = epochs,\n",
    "    validation_data = (X_train[val], y_train[val]),\n",
    "    verbose = 1,\n",
    "    steps_per_epoch = X_train[train].shape[0] // batch_size,\n",
    "    callbacks = callbacks_list\n",
    ")\n",
    "\n",
    "#modelQ4.save(f'data_models/224x224_bs8_300fine_{model_no}.h5', overwrite=True)\n",
    "# serialize model to JSON\n",
    "classifier_json = modelQ4.to_json()\n",
    "with open(f\"../models/models_jsons/MODEL_NAME_{model_no}.json\", \"w\") as json_file:\n",
    "    json_file.write(classifier_json)\n",
    "# serialize weights to HDF5\n",
    "modelQ4.save_weights(f\"../models/model_weights/MODEL_NAME_{model_no}.weights.h5\")\n",
    "print(\"Saved weights to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a59fa-6f80-4963-a68b-31ad373863cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold Cross Validation model evaluation\n",
    "batch_size = 4\n",
    "\n",
    "for (train, val), model_no in zip((kfold.split(X_train, y_train)), range(1,6)):\n",
    "\n",
    "    base_model = keras.applications.Xception(\n",
    "        weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "        input_shape=input_shape,\n",
    "        include_top=False)\n",
    "   \n",
    "    ## callbacks part\n",
    "    mc_path = f\"../models/best_models/{base_model.name}_kfold-model{model_no}.weights.h5\"\n",
    "   \n",
    "    model_checkpoint = ModelCheckpoint(filepath = mc_path,\n",
    "                                   monitor = 'val_accuracy',\n",
    "                                   mode = 'max',\n",
    "                                   verbose = 1,\n",
    "                                   save_best_only = True,\n",
    "                                   save_weights_only = True)\n",
    "   \n",
    "   \n",
    "    learning_rate_reduction = ReduceLROnPlateau(\n",
    "        monitor='val_accuracy',\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        factor=0.1,\n",
    "        min_lr=0.00001\n",
    "    )\n",
    "   \n",
    "    log_csv = CSVLogger(f'../models/model_logs/{base_model.name}_model{model_no}_logs.csv', separator = ',', append = False)\n",
    "\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience = 10)\n",
    "    \n",
    "    callbacks_list = [learning_rate_reduction, model_checkpoint, log_csv]\n",
    "   \n",
    "   \n",
    "    base_modelQ4 = base_model\n",
    "   \n",
    "    base_modelQ4.trainable = False\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    \n",
    "   \n",
    "    q4 = base_modelQ4(inputs, training=False)\n",
    "    q4 = keras.layers.Conv2D(64, (1,1), activation = 'relu', padding = 'same', name = \"pikachu\")(q4)\n",
    "    q4 = keras.layers.GlobalAveragePooling2D()(q4)\n",
    "    q4 = keras.layers.Dropout(0.2)(q4)\n",
    "    q4 = keras.layers.Flatten()(q4)\n",
    "    q4 = keras.layers.Dense(256, activation='relu')(q4)\n",
    "    q4 = keras.layers.Dropout(0.2)(q4)\n",
    "    q4 = keras.layers.Dense(128, activation='relu')(q4)\n",
    "    q4 = keras.layers.Dropout(0.3)(q4)\n",
    "    q4 = keras.layers.Dense(64, activation='relu')(q4)\n",
    "    q4 = keras.layers.Dropout(0.3)(q4)\n",
    "    \n",
    "    outputs = keras.layers.Dense(num_classes, activation='softmax')(q4)\n",
    "    modelQ4 = keras.Model(inputs, outputs)\n",
    "   \n",
    "   \n",
    "    modelQ4.compile(loss= loss_function,\n",
    "                    metrics=[\"accuracy\"],\n",
    "                    optimizer=optimizer)\n",
    "\n",
    "    print(y_train.shape)\n",
    "   \n",
    "    epochs = 50\n",
    "    modelQ4.fit(train_datagen.flow(X_train[train], y_train[train], batch_size = batch_size),\n",
    "                          epochs=epochs,\n",
    "                          validation_data = (X_train[val], y_train[val]),\n",
    "                          verbose=1,\n",
    "                          steps_per_epoch = X_train[train].shape[0] // batch_size,\n",
    "                          callbacks = [learning_rate_reduction])\n",
    "   \n",
    "    # Fine Tuning\n",
    "    base_modelQ4.trainable = True\n",
    "    modelQ4.compile(loss= loss_function,\n",
    "                    metrics=[\"accuracy\"],\n",
    "                    optimizer = optimizer)\n",
    "   \n",
    "   \n",
    "    epochs = 100\n",
    "    callbacks_list = [learning_rate_reduction, model_checkpoint, log_csv,early_stopping]\n",
    "    modelQ4.fit(train_datagen.flow(X_train[train], y_train[train], batch_size = batch_size),\n",
    "                          epochs = epochs,\n",
    "                          validation_data = (X_train[val], y_train[val]),\n",
    "                          verbose=1,\n",
    "                          steps_per_epoch= X_train[train].shape[0] // batch_size,\n",
    "                          callbacks = callbacks_list)\n",
    "    \n",
    "    #modelQ4.save(f'data_models/224x224_bs8_300fine_{model_no}.h5', overwrite=True)\n",
    "    # serialize model to JSON\n",
    "    classifier_json = modelQ4.to_json()\n",
    "    with open(f\"../models/models_jsons/MODEL_NAME_{model_no}.json\", \"w\") as json_file:\n",
    "        json_file.write(classifier_json)\n",
    "    # serialize weights to HDF5\n",
    "    modelQ4.save_weights(f\"../models/model_weights/MODEL_NAME_{model_no}.h5\")\n",
    "    print(\"Saved weights to disk\")\n",
    "   \n",
    "      # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {model_no} ...')\n",
    "     \n",
    "    json_file = open(f'../models/models_jsons/MODEL_NAME_{model_no}.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(f\"../models/best_models/{base_model.name}_kfold-model{model_no}.hdf5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    \n",
    "    loaded_model.compile(loss= loss_function,\n",
    "                    metrics=[\"accuracy\"],\n",
    "                    optimizer = optimizer)\n",
    "   \n",
    "    \n",
    "    # Generate generalization metrics\n",
    "    scores = loaded_model.evaluate(X_train[val], y_train[val], verbose = 0)\n",
    "    print(f'Score for fold {model_no}: {loaded_model.metrics_names[0]} of {scores[0]}; {loaded_model.metrics_names[1]} of {scores[1]*100}%\\n\\n')\n",
    "    val_acc_per_fold.append(scores[1] * 100)\n",
    "    val_loss_per_fold.append(scores[0])\n",
    "   \n",
    "       \n",
    "    test_loss_eval, test_accuracy_eval = loaded_model.evaluate(X_test, y_test, verbose = 0)\n",
    "    print(\"Eval: accuracy = %f  ;  loss = %f\" % (test_accuracy_eval, test_loss_eval))\n",
    "           \n",
    "    test_acc_per_fold_eva.append(test_accuracy_eval)\n",
    "    test_loss_per_fold_eva.append(test_loss_eval)    \n",
    "\n",
    "\n",
    "    Y_pred = loaded_model.predict(X_test)\n",
    "    Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "    Y_true = np.argmax(y_test, axis=1)\n",
    "    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
    "    plot_confusion_matrix(confusion_mtx, classes=classes, model_no = model_no)  \n",
    "   \n",
    "    test_accuracy_pred = accuracy_score(Y_true, Y_pred_classes)\n",
    "   \n",
    "    print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(Y_true, Y_pred_classes)))\n",
    "   \n",
    "    print('Micro Precision: {:.2f}'.format(precision_score(Y_true, Y_pred_classes, average='micro')))\n",
    "    print('Micro Recall: {:.2f}'.format(recall_score(Y_true, Y_pred_classes, average='micro')))\n",
    "    print('Micro F1-score: {:.2f}\\n'.format(f1_score(Y_true, Y_pred_classes, average='micro')))\n",
    "   \n",
    "    print('Macro Precision: {:.2f}'.format(precision_score(Y_true, Y_pred_classes, average='macro')))\n",
    "    print('Macro Recall: {:.2f}'.format(recall_score(Y_true, Y_pred_classes, average='macro')))\n",
    "    print('Macro F1-score: {:.2f}\\n'.format(f1_score(Y_true, Y_pred_classes, average='macro')))\n",
    "   \n",
    "    print('Weighted Precision: {:.2f}'.format(precision_score(Y_true, Y_pred_classes, average='weighted')))\n",
    "    print('Weighted Recall: {:.2f}'.format(recall_score(Y_true, Y_pred_classes, average='weighted')))\n",
    "    print('Weighted F1-score: {:.2f}'.format(f1_score(Y_true, Y_pred_classes, average='weighted')))\n",
    "   \n",
    "    test_acc_per_fold_pred.append(test_accuracy_pred)    \n",
    "   \n",
    "    print('\\nClassification Report\\n')\n",
    "    print(classification_report(Y_true, Y_pred_classes, target_names= classes, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804de0ad-2dc9-4d01-a68d-9bfae6407b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(test_acc_per_fold_pred)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1}  - Accuracy: {test_acc_per_fold_pred[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(test_acc_per_fold_pred)} (+- {np.std(test_acc_per_fold_pred)})')\n",
    "#print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "#%% 1 genereal model\n",
    "\n",
    "base_model = keras.applications.Xception(  # Xception, VGG16, MobileNetV2,ResNet50V2,ResNet101V2 //InceptionResNetV2 299 ,EfficientNetV2L\n",
    "       weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "       input_shape=input_shape,\n",
    "       include_top=False)\n",
    "\n",
    "   ## callbacks part\n",
    "mc_path = f\"../models/best_models/{base_model.name}.hdf5\"\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(filepath=mc_path,\n",
    "                                      monitor='val_accuracy',\n",
    "                                      mode='max',\n",
    "                                      verbose=1,\n",
    "                                      save_best_only=True,\n",
    "                                      save_weights_only=True)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                                               patience=2,\n",
    "                                               verbose=1,\n",
    "                                               factor=0.1,\n",
    "                                               min_lr=0.00001)\n",
    "\n",
    "log_csv = CSVLogger(\n",
    "       f'../models/model_logs/{base_model.name}_logs.csv', separator=',', append=False)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "callbacks_list = [learning_rate_reduction, model_checkpoint, log_csv]\n",
    "\n",
    "base_modelQ4 = base_model\n",
    "\n",
    "base_modelQ4.trainable = False\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "q4 = base_modelQ4(inputs, training=False)\n",
    "q4 = keras.layers.Conv2D(\n",
    "       64, (1, 1), activation='relu', padding='same', name=\"pikachu\")(q4)\n",
    "q4 = keras.layers.GlobalAveragePooling2D()(q4)\n",
    "q4 = keras.layers.Dropout(0.2)(q4)\n",
    "q4 = keras.layers.Flatten()(q4)\n",
    "q4 = keras.layers.Dense(256, activation='relu')(q4)\n",
    "q4 = keras.layers.Dropout(0.2)(q4)\n",
    "q4 = keras.layers.Dense(128, activation='relu')(q4)\n",
    "q4 = keras.layers.Dropout(0.3)(q4)\n",
    "q4 = keras.layers.Dense(64, activation='relu')(q4)\n",
    "q4 = keras.layers.Dropout(0.3)(q4)\n",
    "\n",
    "outputs = keras.layers.Dense(num_classes, activation='softmax')(q4)\n",
    "modelQ4 = keras.Model(inputs, outputs)\n",
    "\n",
    "modelQ4.compile(loss=loss_function,\n",
    "                   metrics=[\"accuracy\"],\n",
    "                   optimizer=optimizer)\n",
    "\n",
    "epochs = 50\n",
    "modelQ4.fit_generator(train_datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                         epochs=epochs,\n",
    "                         validation_data=(X_val, y_val),\n",
    "                         verbose=1,\n",
    "                         steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                         callbacks=[learning_rate_reduction])\n",
    "\n",
    "   # Fine Tuning\n",
    "base_modelQ4.trainable = True\n",
    "modelQ4.compile(loss=loss_function,\n",
    "                   metrics=[\"accuracy\"],\n",
    "                   optimizer=optimizer)\n",
    "\n",
    "epochs = 100\n",
    "callbacks_list = [learning_rate_reduction,\n",
    "                     model_checkpoint, log_csv, early_stopping]\n",
    "modelQ4.fit_generator(train_datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                         epochs=epochs,\n",
    "                         validation_data=(X_val, y_val),\n",
    "                         verbose=1,\n",
    "                         steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                         callbacks=callbacks_list)\n",
    "\n",
    "   #modelQ4.save(f'data_models/224x224_bs8_300fine_{model_no}.h5', overwrite=True)\n",
    "   # serialize model to JSON\n",
    "classifier_json = modelQ4.to_json()\n",
    "with open(f\"../models/models_jsons/224x224_bs8_300fine.json\", \"w\") as json_file:\n",
    "        json_file.write(classifier_json)\n",
    "    # serialize weights to HDF5\n",
    "modelQ4.save_weights(\n",
    "        f\"../models/model_weights/224x224_bs8_300fine.h5\")\n",
    "print(\"Saved weights to disk\")\n",
    "\n",
    "    # Generate a print\n",
    "print('------------------------------------------------------------------------')\n",
    "print(f'Training ')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc4ff7-e088-4396-8908-3d113ddaba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"../models/models_jsons/JSON_NAME.json\"\n",
    "model_path = \"../models/best_models/MODEL_NAME.weights.h5\"\n",
    "\n",
    "json_file = open(json_path, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(model_path)\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "loaded_model.compile(loss=loss_function,\n",
    "                         metrics=[\"accuracy\"],\n",
    "                         optimizer=optimizer)\n",
    "\n",
    "# Generate generalization metrics\n",
    "scores = loaded_model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f'Score: {loaded_model.metrics_names[0]} of {scores[0]}; {loaded_model.metrics_names[1]} of {scores[1]*100}%\\n\\n')\n",
    "\n",
    "\n",
    "test_loss_eval, test_accuracy_eval = loaded_model.evaluate(\n",
    "        X_test, y_test, verbose=0)\n",
    "print(\"Eval: accuracy = %f  ;  loss = %f\" %\n",
    "          (test_accuracy_eval, test_loss_eval))\n",
    "\n",
    "\n",
    "Y_pred = loaded_model.predict(X_test)\n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "Y_true = np.argmax(y_test, axis=1)\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
    "plot_confusion_matrix(confusion_mtx, classes=classes)\n",
    "\n",
    "test_accuracy_pred = accuracy_score(Y_true, Y_pred_classes)\n",
    "\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(\n",
    "        accuracy_score(Y_true, Y_pred_classes)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(\n",
    "        precision_score(Y_true, Y_pred_classes, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(\n",
    "        recall_score(Y_true, Y_pred_classes, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(Y_true,\n",
    "          Y_pred_classes, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(\n",
    "        precision_score(Y_true, Y_pred_classes, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(\n",
    "        recall_score(Y_true, Y_pred_classes, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(Y_true,\n",
    "          Y_pred_classes, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(\n",
    "        precision_score(Y_true, Y_pred_classes, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(\n",
    "        recall_score(Y_true, Y_pred_classes, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(Y_true,\n",
    "          Y_pred_classes, average='weighted')))\n",
    "\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(\n",
    "    Y_true,\n",
    "    Y_pred_classes,\n",
    "    target_names=classes,\n",
    "    digits=4\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef57e19-d532-4b0f-93b8-9b902108e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def enable_mc_dropout(model):\n",
    "    \"\"\"Modifies the model to enable Monte Carlo dropout at inference time.\"\"\"\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Dropout):\n",
    "            layer.training = True\n",
    "    return model\n",
    "\n",
    "def mc_dropout_predictions(model, X, n_simulations=50):\n",
    "    \"\"\"Runs multiple forward passes with MC dropout to estimate uncertainty.\"\"\"\n",
    "    preds = np.array([model.predict(X, verbose=0) for _ in range(n_simulations)])\n",
    "\n",
    "    for prd in preds:\n",
    "        print(prd)\n",
    "    \n",
    "    mean_pred = preds.mean(axis=0)  # Mean prediction\n",
    "    std_pred = preds.std(axis=0)    # Standard deviation (uncertainty measure)\n",
    "    return mean_pred, std_pred\n",
    "\n",
    "# Load the model architecture\n",
    "json_path = \"../models/models_jsons/JSON_NAME.json\"\n",
    "model_path = \"../models/best_models/MODEL_NAME.weights.h5\"\n",
    "\n",
    "with open(json_path, 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(model_path)\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# Compile the model\n",
    "loaded_model.compile(loss=loss_function, metrics=[\"accuracy\"], optimizer=optimizer)\n",
    "\n",
    "# Evaluate standard performance\n",
    "scores = loaded_model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f'Score: {loaded_model.metrics_names[0]} of {scores[0]}; {loaded_model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "test_loss_eval, test_accuracy_eval = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Eval: accuracy = {test_accuracy_eval:.6f}  ;  loss = {test_loss_eval:.6f}\")\n",
    "\n",
    "# Enable MC Dropout\n",
    "mc_model = enable_mc_dropout(loaded_model)\n",
    "\n",
    "# Monte Carlo Dropout Predictions\n",
    "Y_pred_mean, Y_pred_std = mc_dropout_predictions(mc_model, X_test, n_simulations=100)\n",
    "Y_pred_classes = np.argmax(Y_pred_mean, axis=1)\n",
    "Y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Compute Uncertainty (Standard deviation per sample)\n",
    "uncertainty = np.mean(Y_pred_std, axis=1)\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
    "plot_confusion_matrix(confusion_mtx, classes=classes)\n",
    "\n",
    "test_accuracy_pred = accuracy_score(Y_true, Y_pred_classes)\n",
    "print(f'\\nAccuracy: {test_accuracy_pred:.2f}\\n')\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(Y_true, Y_pred_classes, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(Y_true, Y_pred_classes, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(Y_true, Y_pred_classes, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(Y_true, Y_pred_classes, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(Y_true, Y_pred_classes, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(Y_true, Y_pred_classes, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(Y_true, Y_pred_classes, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(Y_true, Y_pred_classes, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}\\n'.format(f1_score(Y_true, Y_pred_classes, average='weighted')))\n",
    "\n",
    "# Classification Report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(Y_true, Y_pred_classes, target_names=classes, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
